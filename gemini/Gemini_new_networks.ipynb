{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a133c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from func import out_string_nets, textread, out_moment_emb\n",
    "from mashup import mashup_multi, load_multi\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13101b",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde5f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these two according to your dataset\n",
    "org = 'yeast'\n",
    "net = 'mashup'\n",
    "# org = ''\n",
    "# net = ''\n",
    "\n",
    "string_nets = out_string_nets(net, org)\n",
    "string_nets = ['coexpression',\n",
    "             'cooccurence',\n",
    "             'neighborhood']\n",
    "# change string_nets according to your dataset \n",
    "# string_nets = []\n",
    "\n",
    "# you can download the sample data by using 'sh get_data_mashup.sh'\n",
    "# and check the format of xxx_adjacency.txt and xxx_genes.txt\n",
    "# then put your data under the data/networks\n",
    "# Load networks\n",
    "network_files = []\n",
    "for i in range(len(string_nets)):\n",
    "    network_files.append(\n",
    "        f'../data/networks/{org}/{org}_string_{string_nets[i]}_adjacency.txt')\n",
    "\n",
    "# Load gene list\n",
    "gene_file = f'../data/networks/{org}/{org}_{net}_genes.txt'\n",
    "genes = textread(gene_file)\n",
    "ngene = len(genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5518ed7",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81606f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Gemini'\n",
    "mixup = True\n",
    "\n",
    "# torch_thread*num_thread is the total threads to be used\n",
    "num_thread = 2\n",
    "torch_thread = 4\n",
    "\n",
    "# This is a hyperparameter, larger ndim has more representation power.\n",
    "ndim = 400\n",
    "\n",
    "# Change to your desired device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7553ce",
   "metadata": {},
   "source": [
    "## Calculate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network integration\n",
    "# calculate embedding for each networks using random walk with restarting (rwr)\n",
    "weights = None\n",
    "node_weights = None\n",
    "rwr = 'rwr'\n",
    "start_time = time.time()\n",
    "print(f'{method}_{org}_{net}_{ndim}')\n",
    "print('[Mashup]')\n",
    "\n",
    "embd_name = f'../data/embed/{method}_{org}_{net}_{ndim}'\n",
    "x = mashup_multi(network_files, ngene, ndim,\n",
    "                 mixup, num_thread, torch_thread,\n",
    "                 weights,\n",
    "                 node_weights=node_weights,\n",
    "                 rwr=rwr, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a59027",
   "metadata": {},
   "source": [
    "## Calculate embedding for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_type = 'Qsm4' # 4th standard moment on Q(rwr)\n",
    "axis = 1\n",
    "level = 'network' # network or node, we averaged on network in our research\n",
    "average_type = 0 # 0 means on rwr, 1 means on log(rwr), we only implement 0 in this project\n",
    "\n",
    "embeds = []\n",
    "embed_name = f'data/embed/{net}_{org}_type{average_type}_' + \\\n",
    "    f'{embed_type}{axis}_{level}.npy'\n",
    "\n",
    "data = network_files, average_type, ngene\n",
    "f = partial(out_moment_emb, data)\n",
    "num_net = len(network_files)\n",
    "max_len = num_net\n",
    "\n",
    "max_idx = max_len//num_thread\n",
    "max_idx = max_idx+1 if max_len % num_thread > 0 else max_idx\n",
    "for i in tqdm(range(max_idx)):\n",
    "    start_idx = num_thread*(i)\n",
    "    end_idx = num_thread*(i+1)\n",
    "    end_idx = end_idx if end_idx <= max_len else max_len\n",
    "    idxs = np.arange(num_net)[start_idx:end_idx]\n",
    "    use_pool = True\n",
    "    if not use_pool:\n",
    "        embed_part = []\n",
    "        for idx in idxs:\n",
    "            embed_part.append(f(idx))\n",
    "    else:\n",
    "        with Pool(processes=num_thread) as pl:\n",
    "            embed_part = pl.map(f, idxs)\n",
    "    embeds.extend(embed_part)\n",
    "\n",
    "    \n",
    "i_ = -1\n",
    "for od in [1, 2, 3, 4]:\n",
    "    for em in ['Q']:\n",
    "        for embed_type in [f'{em}sm{od}', f'{em}m{od}']:\n",
    "            for axis in [1]:\n",
    "                i_ += 1\n",
    "                c = np.array([embeds[i][i_]\n",
    "                              for i in range(len(embeds))])\n",
    "                np.save(\n",
    "                    f'../data/embed/{net}_{org}_type{average_type}_' +\n",
    "                    f'{embed_type}{axis}_{level}', c)\n",
    "c = np.load(\n",
    "    f'../data/embed/{net}_{org}_type{average_type}_' +\n",
    "    f'{embed_type}{axis}_{level}.npy')\n",
    "c = c[:len(network_files)]\n",
    "print(c.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be4f9f",
   "metadata": {},
   "source": [
    "## Clustering: Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 'ap'\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "print('run AffinityPropagation')\n",
    "clustering = AffinityPropagation(\n",
    "    damping=0.875, random_state=0).fit(c)\n",
    "separate = clustering.labels_\n",
    "\n",
    "print(separate)\n",
    "print(set(separate))\n",
    "print(len(set(separate)))\n",
    "num2i = {num: i for i, num in enumerate(list(set(separate)))}\n",
    "separate = [num2i[num] for num in separate]\n",
    "if not os.path.exists('../data/separate'):\n",
    "    os.mkdir('../data/separate')\n",
    "np.save(\n",
    "    f'../data/separate/{net}_{org}_type{average_type}_' +\n",
    "    f'{embed_type}{axis}_{cluster}_{level}',\n",
    "    separate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1 # using clster size to set weight to each network\n",
    "ori_weight = 0.5\n",
    "ori_seed = 1\n",
    "mixup2 = 1.0\n",
    "mixup = 1\n",
    "gamma = 0.5\n",
    "\n",
    "cluster_method = cluster\n",
    "weights = np.zeros(len(network_files))\n",
    "embed_type = embed_type\n",
    "axis = 1\n",
    "separate = np.load(\n",
    "    f'../data/separate/{net}_{org}_type0_' +\n",
    "    f'{embed_type}{axis}_{cluster}_' +\n",
    "    f'{level}.npy')\n",
    "if weight == 2:\n",
    "    clus_count = np.ones(len(set(separate)))\n",
    "elif weight == 1:\n",
    "    clus_count = np.zeros(len(set(separate)))\n",
    "separate = separate[:len(network_files)]\n",
    "for i in separate:\n",
    "    clus_count[i] += 1\n",
    "if weight == 2:\n",
    "    clus_weight = 1/clus_count + \\\n",
    "        ori_weight/len(network_files)\n",
    "elif weight == 1:\n",
    "    clus_weight = 1/clus_count\n",
    "weights += np.array([clus_weight[i] for i in separate])\n",
    "\n",
    "\n",
    "embd_name += f'_{embed_type}{axis}_' + \\\n",
    "    f'separate{separate}_{cluster_method}' + \\\n",
    "    f'_weight{weight}_{ori_weight}'\n",
    "separate = None\n",
    "print(level)\n",
    "embd_name += f'_{level}'\n",
    "\n",
    "if mixup > 0:\n",
    "    network_pairs_mixup_ = []\n",
    "    from numpy.random import choice\n",
    "    random.seed(1)\n",
    "    print(ndim)\n",
    "    # np.random.seed(1)\n",
    "    p = weights\n",
    "    p = p/p.sum()\n",
    "    list_of_candidates = np.arange(len(network_files))\n",
    "    for idd in range(mixup):\n",
    "        network_pairs_mixup = []\n",
    "        ori_seed = int(np.floor(ori_seed*10000)/10000)\n",
    "        np.random.seed(idd+ori_seed)\n",
    "        for _ in range(round(len(network_files)*mixup2)):\n",
    "            # for ixd in range(mixup):\n",
    "            draw = choice(list_of_candidates, 2,\n",
    "                          p=p)\n",
    "            d0, d1 = draw[0], draw[1]\n",
    "            # if separate[d0] != separate[d1]:\n",
    "            n0 = network_files[d0]\n",
    "            n1 = network_files[d1]\n",
    "            network_pairs_mixup.append([n0, 1, n1, 1])\n",
    "        network_pairs_mixup_.append(network_pairs_mixup)\n",
    "    mixup = 'mixup'\n",
    "    embd_name += f'_mixup{mixup}_{mixup2}'\n",
    "    embd_name += f'_gamma{gamma}'\n",
    "    network_files_all = network_pairs_mixup_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05351a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using multiply time mixup to make new embeding')\n",
    "xs = []\n",
    "for network_files in network_files_all:\n",
    "    xs.append(load_multi(network_files, ngene, ndim,\n",
    "                         mixup, num_thread,\n",
    "                         torch_thread,\n",
    "                         weights,\n",
    "                         node_weights=node_weights,\n",
    "                         gamma=gamma, device=device))\n",
    "if len(xs) > 0:\n",
    "    x = np.concatenate(xs, axis=0)\n",
    "np.save(embd_name, x)\n",
    "# x is the embedding we need"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
